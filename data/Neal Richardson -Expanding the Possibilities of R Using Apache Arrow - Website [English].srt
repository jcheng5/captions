1
00:00:07,160 --> 00:00:08,910
NEAL RICHARDSON: You
know that feeling you

2
00:00:08,910 --> 00:00:11,490
get when you've got these
really, really good results,

3
00:00:11,490 --> 00:00:14,083
and you're just really excited
to share them with the world

4
00:00:14,083 --> 00:00:16,500
and you think everyone's going
to be really excited to see

5
00:00:16,500 --> 00:00:18,596
what results you have?

6
00:00:18,596 --> 00:00:20,770
And then you get
that other feeling

7
00:00:20,770 --> 00:00:23,940
you sometimes get with that
one when you get this sinking

8
00:00:23,940 --> 00:00:27,600
suspicion that maybe those
really good results are just

9
00:00:27,600 --> 00:00:29,540
too good to be true.

10
00:00:29,540 --> 00:00:32,200
So this happened to
me a few months ago.

11
00:00:32,200 --> 00:00:34,360
I lead the engineering
team at Ursa Labs.

12
00:00:34,360 --> 00:00:37,900
We're one of the leading
developers behind the Apache

13
00:00:37,900 --> 00:00:39,340
Arrow project.

14
00:00:39,340 --> 00:00:42,100
So Arrow, as you may
know, is essentially

15
00:00:42,100 --> 00:00:45,370
building the foundation for the
next generation of data frames

16
00:00:45,370 --> 00:00:47,440
and bigger data analysis.

17
00:00:47,440 --> 00:00:50,180
And I maintain
arrow's R package.

18
00:00:50,180 --> 00:00:52,780
So I was meeting with
RStudio's leadership

19
00:00:52,780 --> 00:00:56,410
because RStudio has been a
generous sponsor of Ursa Labs

20
00:00:56,410 --> 00:00:58,640
from the beginning.

21
00:00:58,640 --> 00:01:03,430
So I'm in the call, and I'm
there with Hadley Wickham

22
00:01:03,430 --> 00:01:07,180
and with JJ Allaire,
founder and CEO of RStudio,

23
00:01:07,180 --> 00:01:09,040
Tareef Kawaf, who's president.

24
00:01:09,040 --> 00:01:10,810
And I was talking
about this result

25
00:01:10,810 --> 00:01:14,220
that I found recently, which was
I've been benchmarking Arrow's

26
00:01:14,220 --> 00:01:17,983
CSV reader and found that
Arrow has lots of great things

27
00:01:17,983 --> 00:01:19,150
that it does in its library.

28
00:01:19,150 --> 00:01:21,310
I'm going to show you
some of them in this talk.

29
00:01:21,310 --> 00:01:24,130
But even if you just use
arrow as a CSV reader

30
00:01:24,130 --> 00:01:26,440
and use it to read
a data frame into R

31
00:01:26,440 --> 00:01:29,140
and then you could use any other
R package on that data frame

32
00:01:29,140 --> 00:01:31,810
afterwards, you can
get a big benefit.

33
00:01:31,810 --> 00:01:34,930
And in fact, even, on this
particular New York City taxi

34
00:01:34,930 --> 00:01:37,630
dataset that I was
testing out on,

35
00:01:37,630 --> 00:01:40,630
I found that Arrow
was two to three times

36
00:01:40,630 --> 00:01:44,080
faster than data.table's
CSV reader.

37
00:01:44,080 --> 00:01:46,940
And at this point, Hadley
cuts me off and says,

38
00:01:46,940 --> 00:01:47,778
yeah, I don't know.

39
00:01:47,778 --> 00:01:48,820
That doesn't sound right.

40
00:01:48,820 --> 00:01:51,980
I mean, data.table,
that group, they

41
00:01:51,980 --> 00:01:55,790
have optimized really everything
that you could optimize out

42
00:01:55,790 --> 00:02:01,220
of reading data into
R. And so maybe you

43
00:02:01,220 --> 00:02:03,830
could match their performance,
maybe get a little bit better,

44
00:02:03,830 --> 00:02:05,997
but you couldn't be that
much better because they've

45
00:02:05,997 --> 00:02:07,370
done everything you could do.

46
00:02:07,370 --> 00:02:09,889
And Tareef added, yeah, you
probably screwed up something

47
00:02:09,889 --> 00:02:11,030
in your benchmarking code.

48
00:02:11,030 --> 00:02:13,580
You know, you probably
actually didn't do all the work

49
00:02:13,580 --> 00:02:15,018
you thought you did.

50
00:02:15,018 --> 00:02:16,810
And I thought about
it, and it's like, wow,

51
00:02:16,810 --> 00:02:20,260
that's actually a much more
logical, simpler explanation

52
00:02:20,260 --> 00:02:22,445
for the result that I
found-- that I screwed up

53
00:02:22,445 --> 00:02:23,320
something in my code.

54
00:02:23,320 --> 00:02:28,240
But I figured I'd go check
it out and get back to them.

55
00:02:28,240 --> 00:02:31,090
And I compared the
objects that I had.

56
00:02:31,090 --> 00:02:34,000
The data frames read by
both were the same size--

57
00:02:34,000 --> 00:02:37,090
15 million rows, both
really big in memory.

58
00:02:37,090 --> 00:02:38,050
So it checked out.

59
00:02:38,050 --> 00:02:40,780
It turned out Arrow's CSV
reader was two or three times

60
00:02:40,780 --> 00:02:43,970
faster than data table's here.

61
00:02:43,970 --> 00:02:46,690
So we'd achieved an
incredible result.

62
00:02:46,690 --> 00:02:48,010
It was literally incredible.

63
00:02:48,010 --> 00:02:49,810
Like, this group
of people here that

64
00:02:49,810 --> 00:02:52,240
know a lot about
the internals of R

65
00:02:52,240 --> 00:02:55,540
literally did not believe
that it was this fast.

66
00:02:55,540 --> 00:02:58,410
So how are we able to do that?

67
00:02:58,410 --> 00:03:02,540
So what I want to talk about
today is how characteristics

68
00:03:02,540 --> 00:03:05,840
of the Arrow project and
particularly the community

69
00:03:05,840 --> 00:03:10,160
around it enable us to do
things that we might otherwise

70
00:03:10,160 --> 00:03:14,930
have thought would be
impossible to do in R.

71
00:03:14,930 --> 00:03:17,000
So a little history--

72
00:03:17,000 --> 00:03:21,500
so Arrow was started in 2016.

73
00:03:21,500 --> 00:03:25,100
A group of database developers
and data frame library

74
00:03:25,100 --> 00:03:27,020
maintainers got
together and realized

75
00:03:27,020 --> 00:03:29,930
that they were all trying
to solve similar problems.

76
00:03:29,930 --> 00:03:32,540
And then rather than saying,
you know, this is fine

77
00:03:32,540 --> 00:03:34,340
and we're just going
to keep duplicating

78
00:03:34,340 --> 00:03:36,710
each other's work, what
if we work together

79
00:03:36,710 --> 00:03:39,710
and create a shared
foundation for our work,

80
00:03:39,710 --> 00:03:42,700
and we would all
benefit from that?

81
00:03:42,700 --> 00:03:45,430
So Arrow is fundamentally
about three things.

82
00:03:45,430 --> 00:03:47,370
First, it is a format.

83
00:03:47,370 --> 00:03:51,660
It is a specification for how
data is represented in memory.

84
00:03:51,660 --> 00:03:53,970
It's columnar, and
it's designed to take

85
00:03:53,970 --> 00:03:56,550
advantage of features
of modern CPUs

86
00:03:56,550 --> 00:04:00,370
and GPUs and other hardware.

87
00:04:00,370 --> 00:04:03,280
There's also a set of 12
libraries that implement it

88
00:04:03,280 --> 00:04:05,110
in different languages.

89
00:04:05,110 --> 00:04:08,860
Python and R use
the C++ library,

90
00:04:08,860 --> 00:04:14,280
but there are many others
in the Arrow project.

91
00:04:14,280 --> 00:04:16,620
And then third, there's
this broader ecosystem

92
00:04:16,620 --> 00:04:20,430
of packages and
projects around Arrow,

93
00:04:20,430 --> 00:04:22,260
that use Arrow in some
form, whether it's

94
00:04:22,260 --> 00:04:25,170
as their internal data
model or as it's just

95
00:04:25,170 --> 00:04:28,884
a means of exchange
between projects.

96
00:04:28,884 --> 00:04:33,260
So how do these
characteristics of arrow

97
00:04:33,260 --> 00:04:36,470
lead us to get such
unbelievable performance

98
00:04:36,470 --> 00:04:38,750
and be able to enable us
to do things that we didn't

99
00:04:38,750 --> 00:04:41,850
think were possible in R?

100
00:04:41,850 --> 00:04:44,625
So in terms of modern
hardware, arrow

101
00:04:44,625 --> 00:04:47,700
is designed to take advantage
of many features of CPUs

102
00:04:47,700 --> 00:04:52,380
that exist now that didn't exist
when R was developed initially.

103
00:04:52,380 --> 00:04:55,910
So R was first released in 1995.

104
00:04:55,910 --> 00:04:58,670
Of course, computer technology
has progressed quite a bit

105
00:04:58,670 --> 00:04:59,720
since then.

106
00:04:59,720 --> 00:05:02,330
And there are things that
I can do on my laptop now

107
00:05:02,330 --> 00:05:06,530
that was not something
that was a thing back then.

108
00:05:06,530 --> 00:05:10,760
So for example, my laptop
here's got eight course.

109
00:05:10,760 --> 00:05:13,130
That's CPUs that you
can run in parallel.

110
00:05:13,130 --> 00:05:17,180
You can get a host
on AWS with 96 cores.

111
00:05:17,180 --> 00:05:18,500
That's pretty good.

112
00:05:18,500 --> 00:05:20,700
But R is generally only
using one of those.

113
00:05:20,700 --> 00:05:24,320
So we're leaving a lot of
performance on the table.

114
00:05:24,320 --> 00:05:26,420
Newer CPUs also
can take advantage

115
00:05:26,420 --> 00:05:27,650
of what's called SIMD--

116
00:05:27,650 --> 00:05:29,990
Single Instruction,
Multiple Data.

117
00:05:29,990 --> 00:05:33,840
My laptop, for example, can
take up to 256 bits at a time.

118
00:05:33,840 --> 00:05:36,650
So that would be the size
of eight integers in R.

119
00:05:36,650 --> 00:05:39,270
Other newer ones
can do 512 bits.

120
00:05:39,270 --> 00:05:42,350
So you can feed a lot
more data into the CPU

121
00:05:42,350 --> 00:05:44,570
at a time if your
code is designed

122
00:05:44,570 --> 00:05:45,900
to take advantage of that.

123
00:05:45,900 --> 00:05:48,300
But R generally is not.

124
00:05:48,300 --> 00:05:51,810
So it's like you have this
super fast sports car,

125
00:05:51,810 --> 00:05:54,570
and you just want to take it
for a spin around the track

126
00:05:54,570 --> 00:05:56,560
and see what you can do with it.

127
00:05:56,560 --> 00:06:00,150
But for whatever
reason, you have

128
00:06:00,150 --> 00:06:03,630
to keep it under
25 miles an hour,

129
00:06:03,630 --> 00:06:05,340
drive, keep your hands
at 10:00 and 2:00

130
00:06:05,340 --> 00:06:07,920
on the handle on
the steering wheel--

131
00:06:07,920 --> 00:06:09,240
nothing too crazy.

132
00:06:09,240 --> 00:06:11,582
And it's a really
missed opportunity.

133
00:06:11,582 --> 00:06:12,790
So what do you do about that?

134
00:06:12,790 --> 00:06:18,893
So you could write some C or
C++ code in your R package to do

135
00:06:18,893 --> 00:06:21,060
multi-threading to take
advantage of all those extra

136
00:06:21,060 --> 00:06:23,810
cores that you have available.

137
00:06:23,810 --> 00:06:27,810
You could write code to
do some de-optimizations.

138
00:06:27,810 --> 00:06:32,693
But that's really hard,
frankly, and the number

139
00:06:32,693 --> 00:06:34,110
of people that
know how to do that

140
00:06:34,110 --> 00:06:37,260
well isn't that great probably.

141
00:06:37,260 --> 00:06:40,470
And the number of people
who do that and are

142
00:06:40,470 --> 00:06:44,050
going to be writing R
packages is even smaller.

143
00:06:44,050 --> 00:06:47,670
So here's where the large Arrow
community comes in hand it

144
00:06:47,670 --> 00:06:49,680
here.

145
00:06:49,680 --> 00:06:53,770
So the Arrow project has had,
since it was created in 2016,

146
00:06:53,770 --> 00:06:56,400
has had a steady
growth of contributors

147
00:06:56,400 --> 00:07:00,900
to the project over time,
well over 500 by now.

148
00:07:00,900 --> 00:07:03,030
And what this means
is all of these people

149
00:07:03,030 --> 00:07:06,030
are working together
on this project that--

150
00:07:06,030 --> 00:07:07,750
and the benefits are shared.

151
00:07:07,750 --> 00:07:12,690
So in order to have smart
multi-threading in the R

152
00:07:12,690 --> 00:07:14,610
package, I didn't
have to write that.

153
00:07:14,610 --> 00:07:17,193
Someone else who understands how
to do that much better than I

154
00:07:17,193 --> 00:07:18,480
would did that.

155
00:07:18,480 --> 00:07:20,543
I didn't have to write
SIMD code either.

156
00:07:20,543 --> 00:07:21,960
There are other
people, people who

157
00:07:21,960 --> 00:07:26,490
work on hardware, that's
the actual CPUs themselves,

158
00:07:26,490 --> 00:07:28,890
that know what
those options are.

159
00:07:28,890 --> 00:07:30,282
They wrote that code.

160
00:07:30,282 --> 00:07:31,740
And they're all
contributing it in.

161
00:07:31,740 --> 00:07:33,323
And they're not all
contributing it in

162
00:07:33,323 --> 00:07:35,880
because they want to make
the R package faster.

163
00:07:35,880 --> 00:07:40,230
They're contributing because
they are using the C++ library

164
00:07:40,230 --> 00:07:43,080
for some other project,
or they're using Python,

165
00:07:43,080 --> 00:07:46,230
which uses the same C++
library that the R package does.

166
00:07:46,230 --> 00:07:49,620
So we benefit from all of
these other communities'

167
00:07:49,620 --> 00:07:51,840
contributions.

168
00:07:51,840 --> 00:07:54,130
So what does that look like?

169
00:07:54,130 --> 00:07:58,890
So to give an example of how
this work plays out in reality,

170
00:07:58,890 --> 00:08:00,870
I want to talk
about this example

171
00:08:00,870 --> 00:08:04,680
this demo that I gave last
year at RStudio conference.

172
00:08:04,680 --> 00:08:06,960
I was introducing
the Arrow R package,

173
00:08:06,960 --> 00:08:11,570
and I was going
through this example

174
00:08:11,570 --> 00:08:17,180
with the data set where you take
about 10 and 1/2 years of New

175
00:08:17,180 --> 00:08:19,790
York City taxi data,
2 billion rows,

176
00:08:19,790 --> 00:08:23,190
and you could scan that and
to get some results on that

177
00:08:23,190 --> 00:08:24,660
on your laptop.

178
00:08:24,660 --> 00:08:27,890
So I did some kind
of sample dplyr,

179
00:08:27,890 --> 00:08:30,830
filtering and selecting and
grouping and aggregating

180
00:08:30,830 --> 00:08:34,190
on this dataset that I
opened with the error package

181
00:08:34,190 --> 00:08:36,380
where I planted a
directory of these files,

182
00:08:36,380 --> 00:08:38,059
and then I could query it.

183
00:08:38,059 --> 00:08:40,400
And I got a result
in four seconds--

184
00:08:40,400 --> 00:08:41,570
over 2 billion rows.

185
00:08:41,570 --> 00:08:44,720
And that was pretty good--
really happy with that.

186
00:08:44,720 --> 00:08:49,160
I did a talk this past summer,
and I redid the result just

187
00:08:49,160 --> 00:08:50,630
on the latest
version of the code.

188
00:08:50,630 --> 00:08:52,490
And it was twice as fast.

189
00:08:52,490 --> 00:08:55,320
And I hadn't really done
anything in the R package

190
00:08:55,320 --> 00:08:55,820
to do that.

191
00:08:55,820 --> 00:08:58,220
This is all based on
improvements in the underlying

192
00:08:58,220 --> 00:09:02,795
C++ code that all the rest of
the community was working on.

193
00:09:02,795 --> 00:09:05,540
Right before I
did this talk now,

194
00:09:05,540 --> 00:09:07,580
I ran the code again
on the latest version,

195
00:09:07,580 --> 00:09:09,560
and it's another 25% faster.

196
00:09:09,560 --> 00:09:13,340
Again, I didn't do anything
to make that happen.

197
00:09:13,340 --> 00:09:17,950
It's just based on the Arrow
community and its work.

198
00:09:17,950 --> 00:09:21,400
So you know, there's
other reasons,

199
00:09:21,400 --> 00:09:24,760
other ways that this ecosystem
plays out and has benefits

200
00:09:24,760 --> 00:09:28,750
for us in the R community.

201
00:09:28,750 --> 00:09:32,560
So when you hear that a
bunch of database developers

202
00:09:32,560 --> 00:09:34,390
got together and decided
that there wasn't

203
00:09:34,390 --> 00:09:38,410
a good standard
for columnar data

204
00:09:38,410 --> 00:09:41,080
and they were going to create
their own standard to unify all

205
00:09:41,080 --> 00:09:43,810
of these, it might
bring this "XKCD" comic

206
00:09:43,810 --> 00:09:48,100
to mind where the
creation of standards

207
00:09:48,100 --> 00:09:51,070
to solve the lack
of standards just

208
00:09:51,070 --> 00:09:55,180
makes more problems,
more competing standards.

209
00:09:55,180 --> 00:09:56,770
And there certainly
is that risk.

210
00:09:56,770 --> 00:10:00,960
But five years on from
the creation of Arrow,

211
00:10:00,960 --> 00:10:04,750
we can see now how the
benefits of this approach

212
00:10:04,750 --> 00:10:06,890
have played out.

213
00:10:06,890 --> 00:10:13,510
So just to give an example, so
suppose I've got data in Spark,

214
00:10:13,510 --> 00:10:18,490
and I want really quick access
to that in Python and R.

215
00:10:18,490 --> 00:10:21,580
What you could do is that you
could, from Spark, you could

216
00:10:21,580 --> 00:10:25,420
write a special adapter for
Python, a special adapter

217
00:10:25,420 --> 00:10:28,630
for R, that understands--

218
00:10:28,630 --> 00:10:32,830
in R's case, it understands
R's vector types

219
00:10:32,830 --> 00:10:36,040
and specific-- the
bits in the vectors

220
00:10:36,040 --> 00:10:37,600
that make up a data frame.

221
00:10:37,600 --> 00:10:41,470
And you have to do the same
thing for Python, for NumPi,

222
00:10:41,470 --> 00:10:43,330
or Pandas.

223
00:10:43,330 --> 00:10:45,700
And Spark and its
Java code would have

224
00:10:45,700 --> 00:10:48,280
to understand how to do this.

225
00:10:48,280 --> 00:10:50,560
I put another Arrow
in the chart there

226
00:10:50,560 --> 00:10:52,640
to connect the interchange
between Python and R.

227
00:10:52,640 --> 00:10:54,640
That's obviously something
you'd want to do too,

228
00:10:54,640 --> 00:10:57,250
and it's another place
where you'd need someone

229
00:10:57,250 --> 00:10:58,990
on the other side of
the language barrier

230
00:10:58,990 --> 00:11:02,510
to understand the
internals of your format.

231
00:11:02,510 --> 00:11:05,110
So that's feasible if this
is all you care about,

232
00:11:05,110 --> 00:11:08,470
but in reality, there's
many more languages,

233
00:11:08,470 --> 00:11:10,690
and there's many more
projects and too many

234
00:11:10,690 --> 00:11:13,430
to fit on the slide.

235
00:11:13,430 --> 00:11:16,150
And as you can imagine, if you
try to draw all of those lines

236
00:11:16,150 --> 00:11:19,070
to connect the various adapters
to the the different things,

237
00:11:19,070 --> 00:11:22,170
it gets crazy really quickly.

238
00:11:22,170 --> 00:11:23,530
So what do you do?

239
00:11:23,530 --> 00:11:25,113
You're obviously not
going to do that.

240
00:11:25,113 --> 00:11:26,500
That's too much work.

241
00:11:26,500 --> 00:11:31,300
And so what you want is to
have a ubiquitous standard

242
00:11:31,300 --> 00:11:33,460
that everyone can
write to and read from.

243
00:11:33,460 --> 00:11:37,090
And then you don't have
to implement everything

244
00:11:37,090 --> 00:11:37,610
every time.

245
00:11:40,870 --> 00:11:45,280
So prior to Arrow, what you do
is you probably just dump out

246
00:11:45,280 --> 00:11:46,120
a CSV.

247
00:11:46,120 --> 00:11:48,340
Every language knows
how to read a CSV.

248
00:11:48,340 --> 00:11:51,760
Every database knows
how to read in a CSV.

249
00:11:51,760 --> 00:11:54,100
But this comes with some costs.

250
00:11:54,100 --> 00:11:56,890
CSVs can be expensive
to read and write

251
00:11:56,890 --> 00:12:01,360
because you have to convert
between a string format on disk

252
00:12:01,360 --> 00:12:07,390
to the arrays of bits
that R and other languages

253
00:12:07,390 --> 00:12:10,270
use in memory to do work.

254
00:12:10,270 --> 00:12:12,160
And that conversion is costly.

255
00:12:12,160 --> 00:12:14,920
And CSV also doesn't have
rich support for types.

256
00:12:14,920 --> 00:12:16,930
You know, you may
have time stamps,

257
00:12:16,930 --> 00:12:19,398
and you don't have
any way of indicating

258
00:12:19,398 --> 00:12:20,440
that this is a timestamp.

259
00:12:20,440 --> 00:12:22,940
You have to guess whether it
should be just a regular string

260
00:12:22,940 --> 00:12:24,190
or a time stamp.

261
00:12:24,190 --> 00:12:26,950
And so there's lots
of costs and penalties

262
00:12:26,950 --> 00:12:28,120
that come from doing this.

263
00:12:28,120 --> 00:12:30,830
But CSV is kind of the
lowest common denominator.

264
00:12:30,830 --> 00:12:33,550
And so that's why
it gets picked up.

265
00:12:33,550 --> 00:12:35,560
But Arrow is kind
of a happier version

266
00:12:35,560 --> 00:12:40,240
of that where you have a
standard, and it's columnar,

267
00:12:40,240 --> 00:12:41,230
and it's binary.

268
00:12:41,230 --> 00:12:45,670
So it matches very closely
with how R or Julia or Spark

269
00:12:45,670 --> 00:12:48,280
or any of these
languages actually

270
00:12:48,280 --> 00:12:50,480
work with data and memory.

271
00:12:50,480 --> 00:12:54,370
So the conversion is
minimal and in some cases

272
00:12:54,370 --> 00:12:56,350
doesn't actually
involve copying memory

273
00:12:56,350 --> 00:12:58,810
because where it
does actually line up

274
00:12:58,810 --> 00:13:00,160
as being the same shape.

275
00:13:00,160 --> 00:13:02,290
So it's very efficient.

276
00:13:02,290 --> 00:13:04,390
And from our perspective
in the R community,

277
00:13:04,390 --> 00:13:06,190
what this means
is once we have--

278
00:13:06,190 --> 00:13:08,680
and now that we have
an Arrow R package,

279
00:13:08,680 --> 00:13:13,270
we have access to all of these
other projects and databases

280
00:13:13,270 --> 00:13:17,530
and languages very efficiently
as long as they also

281
00:13:17,530 --> 00:13:18,760
can write to Arrow.

282
00:13:18,760 --> 00:13:20,260
And over the last
five years, we've

283
00:13:20,260 --> 00:13:22,420
seen an increasing
number of projects

284
00:13:22,420 --> 00:13:24,160
that have picked up
on Arrow for this

285
00:13:24,160 --> 00:13:26,620
because it's really efficient.

286
00:13:26,620 --> 00:13:30,190
So going back to Spark
for our example there,

287
00:13:30,190 --> 00:13:37,180
early on in Arrow's
life, we added support

288
00:13:37,180 --> 00:13:41,080
in PySpark, the
Python Spark library,

289
00:13:41,080 --> 00:13:43,630
to communicate with
Spark, the database,

290
00:13:43,630 --> 00:13:47,030
which is written in Java,
very efficiently using Arrow.

291
00:13:47,030 --> 00:13:52,973
And so you'd get 100-fold
speed up on certain operations,

292
00:13:52,973 --> 00:13:54,640
whether it's with
user defined functions

293
00:13:54,640 --> 00:13:56,848
or whether you're just
pulling a large amount of data

294
00:13:56,848 --> 00:14:02,560
into your Python environment
to do further analysis on it.

295
00:14:02,560 --> 00:14:05,740
And that worked because
Java has an Arrow library,

296
00:14:05,740 --> 00:14:10,590
Python has an arrow library,
and they write the same format.

297
00:14:10,590 --> 00:14:12,750
The beauty of this is
that once we finally

298
00:14:12,750 --> 00:14:14,670
got the R package
going, we could

299
00:14:14,670 --> 00:14:16,240
pick up and do the same thing.

300
00:14:16,240 --> 00:14:19,200
These are both
references to blog posts

301
00:14:19,200 --> 00:14:20,490
on the Arrow website.

302
00:14:20,490 --> 00:14:22,830
So we were able to take
advantage of the same thing.

303
00:14:22,830 --> 00:14:28,020
And because we can read
and write Arrow from R,

304
00:14:28,020 --> 00:14:32,310
we didn't need to do any other
special Spark business in order

305
00:14:32,310 --> 00:14:36,540
to get those 100x speed
ups of pulling data

306
00:14:36,540 --> 00:14:40,080
to and from Spark in R.

307
00:14:40,080 --> 00:14:43,350
So the Arrow format is
designed to take advantage

308
00:14:43,350 --> 00:14:46,230
of modern hardware,
and the power

309
00:14:46,230 --> 00:14:50,100
of the large community of
developers around the Arrow

310
00:14:50,100 --> 00:14:53,520
project really changed
what we're able to do in R

311
00:14:53,520 --> 00:14:55,500
and enable all sorts
of new workflows

312
00:14:55,500 --> 00:15:00,490
that weren't really
possible or feasible before.

313
00:15:00,490 --> 00:15:03,810
So you know, it's nice, like
I showed in the beginning,

314
00:15:03,810 --> 00:15:10,660
to see that arrow makes, like,
reading CSVs into R faster,

315
00:15:10,660 --> 00:15:14,330
and we like seeing that.

316
00:15:14,330 --> 00:15:16,860
But that's really not the goal.

317
00:15:16,860 --> 00:15:19,550
We're not just trying to
shave off a little bit of time

318
00:15:19,550 --> 00:15:22,970
from work that you
already can do now.

319
00:15:22,970 --> 00:15:25,422
We're trying to go beyond that.

320
00:15:25,422 --> 00:15:27,380
We're trying to enable
new workflows that we're

321
00:15:27,380 --> 00:15:30,500
either cost prohibitive
to do before or just

322
00:15:30,500 --> 00:15:34,330
frankly not possible at all.

323
00:15:34,330 --> 00:15:38,800
So there's a lot more coming in
the coming releases this year

324
00:15:38,800 --> 00:15:39,580
in 2021.

325
00:15:39,580 --> 00:15:41,830
We're really excited to be
able to tell you about them

326
00:15:41,830 --> 00:15:43,400
when they come.

327
00:15:43,400 --> 00:15:46,180
But for now, I wanted to
just go back and walk you

328
00:15:46,180 --> 00:15:48,820
through another example of
something you can do today

329
00:15:48,820 --> 00:15:53,590
in the Arrow package that is
a little bit beyond what you

330
00:15:53,590 --> 00:15:57,130
would normally do
in R but can really

331
00:15:57,130 --> 00:16:01,480
have some impossible
seeming results, let's say.

332
00:16:01,480 --> 00:16:09,130
So earlier, I showed reading
a dataset of 125 files of New

333
00:16:09,130 --> 00:16:10,270
York City taxi data.

334
00:16:10,270 --> 00:16:13,210
It was in the Parquet format.

335
00:16:13,210 --> 00:16:14,920
Parquet is a file
format that you

336
00:16:14,920 --> 00:16:17,530
can read in the Arrow package.

337
00:16:17,530 --> 00:16:20,840
It's a columnar, very
efficient file format.

338
00:16:20,840 --> 00:16:23,047
Often, of course, you don't
have Parquet files yet,

339
00:16:23,047 --> 00:16:24,880
and one of the reasons
you want to use Arrow

340
00:16:24,880 --> 00:16:28,030
is to get your less efficient
CSV data into Parquet

341
00:16:28,030 --> 00:16:31,400
so that you can read
it more quickly.

342
00:16:31,400 --> 00:16:35,480
So the Arrow package has some
facilities for doing this.

343
00:16:35,480 --> 00:16:37,660
So here's an example.

344
00:16:37,660 --> 00:16:40,395
I converted some of those
Parquet files back to CVS.

345
00:16:40,395 --> 00:16:43,630
I only did six months of
it because CSVs are big,

346
00:16:43,630 --> 00:16:45,910
and it takes a lot of space.

347
00:16:45,910 --> 00:16:49,660
But just as you can point in
the directory of Parquet files

348
00:16:49,660 --> 00:16:52,750
and say, open dataset,
you can do that with CSVs.

349
00:16:52,750 --> 00:16:56,230
And you can do the same
kinds of queries on them

350
00:16:56,230 --> 00:16:59,860
that I showed with
the Parquet dataset--

351
00:16:59,860 --> 00:17:01,930
selecting and filtering,
grouping, aggregating,

352
00:17:01,930 --> 00:17:04,099
all of these things.

353
00:17:04,099 --> 00:17:06,182
As you see, the
result here, the one I

354
00:17:06,182 --> 00:17:07,599
showed-- it was a
different query.

355
00:17:07,599 --> 00:17:11,710
So it's not apples to apples,
but it took 7 and 1/2 seconds

356
00:17:11,710 --> 00:17:14,680
to do this query over this data.

357
00:17:14,680 --> 00:17:15,849
It's pretty good.

358
00:17:15,849 --> 00:17:19,210
The fact I didn't have to mess
with reading each of the files

359
00:17:19,210 --> 00:17:22,510
individually and doing
things with it, that's good.

360
00:17:22,510 --> 00:17:25,473
But I think we can do better.

361
00:17:25,473 --> 00:17:26,890
So one of the
things we want to do

362
00:17:26,890 --> 00:17:30,110
is we could write the data set
to a more efficient format,

363
00:17:30,110 --> 00:17:32,050
either Parquet, as I
mentioned, or Feather,

364
00:17:32,050 --> 00:17:33,610
which is the Arrow format.

365
00:17:33,610 --> 00:17:37,560
It's literally the
Arrow format on disk.

366
00:17:37,560 --> 00:17:43,710
So we can use similar types of
dplyr style syntax to do this.

367
00:17:43,710 --> 00:17:47,990
I can say group by this payment
type column in the dataset.

368
00:17:47,990 --> 00:17:51,260
That was one of the columns
that I filtered on before.

369
00:17:51,260 --> 00:17:53,690
And in this case, when I'm
writing a dataset, when

370
00:17:53,690 --> 00:17:55,820
I say group_by,
what that means is

371
00:17:55,820 --> 00:17:59,000
I want you to partition
my data by that.

372
00:17:59,000 --> 00:18:00,710
And what that's
going to do is it's

373
00:18:00,710 --> 00:18:03,170
going to write
out separate files

374
00:18:03,170 --> 00:18:05,420
inside different
directories based

375
00:18:05,420 --> 00:18:07,710
on the value of that variable.

376
00:18:07,710 --> 00:18:11,360
So payment type in this dataset
took on values one to five.

377
00:18:11,360 --> 00:18:14,180
So now I've got, inside
my feather taxi directory,

378
00:18:14,180 --> 00:18:16,520
I've got five
subdirectories, one

379
00:18:16,520 --> 00:18:19,160
for each of the
values of payment type

380
00:18:19,160 --> 00:18:23,570
that now have different
Parquet files or feather files,

381
00:18:23,570 --> 00:18:25,050
excuse me.

382
00:18:25,050 --> 00:18:27,360
And what this means
is when I do a query

383
00:18:27,360 --> 00:18:30,645
again on this dataset, I only--

384
00:18:30,645 --> 00:18:33,150
and if I filter on
payment type, I only

385
00:18:33,150 --> 00:18:36,060
have to look at the files
inside that directory that

386
00:18:36,060 --> 00:18:38,503
correspond to the
value of payment type

387
00:18:38,503 --> 00:18:39,420
that I'm filtering on.

388
00:18:39,420 --> 00:18:41,850
I don't even have to
read the other files in

389
00:18:41,850 --> 00:18:43,860
to see if they match
my filter because I've

390
00:18:43,860 --> 00:18:47,520
kind of pre-filtered it
based on this partitioning.

391
00:18:47,520 --> 00:18:49,500
So indeed, if we do the
same query that I just

392
00:18:49,500 --> 00:18:52,290
did on the CSVs but I
do it on the feather

393
00:18:52,290 --> 00:18:55,770
version of the dataset,
it's now 100 times faster

394
00:18:55,770 --> 00:19:00,180
than before-- exact same query
just with a more efficient file

395
00:19:00,180 --> 00:19:04,620
format and taking
advantage of partitioning

396
00:19:04,620 --> 00:19:06,700
in the dataset query.

397
00:19:06,700 --> 00:19:09,010
So this is great.

398
00:19:09,010 --> 00:19:12,340
We're trying to make things
that have been difficult

399
00:19:12,340 --> 00:19:15,400
or impossible possible,
trying to help

400
00:19:15,400 --> 00:19:17,740
you take full advantage
of the sports car

401
00:19:17,740 --> 00:19:22,720
that you've got on your lap
when you're working in R.

402
00:19:22,720 --> 00:19:25,630
But you know, I
really like to think

403
00:19:25,630 --> 00:19:28,720
what we have is not
just any old sports car,

404
00:19:28,720 --> 00:19:32,500
but it's a DeLorean perhaps
with a flux capacitor in it.

405
00:19:32,500 --> 00:19:35,110
And where we're going,
we don't need roads.

406
00:19:35,110 --> 00:19:38,170
We're going beyond
what you could be doing

407
00:19:38,170 --> 00:19:40,780
on your laptop today even.

408
00:19:40,780 --> 00:19:43,590
So thank you very much.
